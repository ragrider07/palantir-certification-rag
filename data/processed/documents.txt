This is a test document about building an AI agent.

[URL: https://learn.palantir.com/page/training-track-data-engineer?utm_source=chatgpt.com]

Data Engineers are responsible for making data assets useful, accessible, and reliable. They own ingesting data from various systems and transforming, cleaning, and enriching data so that it can be used reliably in the Ontology. The quality of downstream decisions, Ontologies, applications, GenAI-driven workflows relies on the work Data Engineers own in preparing data.
Use the resources below for an introduction to core Data Engineer concepts. The items tagged Example are reference workflows you can deploy to your Foundry instance.

[URL: https://learn.palantir.com/app-dev-guide-2023?utm_source=chatgpt.com]

This lesson is a PDF lesson.
Open in new tab

[URL: https://learn.palantir.com/page/exam-guides?utm_source=chatgpt.com]

We have updated the Application Developer and Data Engineer Certifications to cover topics, questions, and screenshots relevant to the platform as of May 6, 2024.
Select an exam guide below or register for a Certification Exam.

[URL: https://learn.palantir.com/page/training-track-application-developer?utm_source=chatgpt.com]

Application Developers in Foundry specialize in creating operational applications on top of the Ontology using tools like Workshop and Quiver. By building efficient and user-friendly applications, Application Developers play a crucial role in helping organizations unlock the full potential of their organizationâ€™s data in the Ontology and drive informed decision-making.
The trainings and reference material below will enable you to gain hands-on experience in creating Ontology entities, designing Workshop applications, and applying write-backs using Actions - all core skills for Application Developers in Foundry.
Use the resources below for an introduction to core Application Developer concepts. The items tagged Example are reference workflows you can deploy to your Foundry instance.

[URL: https://learn.palantir.com/data-engineer-guide-2023?utm_source=chatgpt.com]

This lesson is a PDF lesson.
Open in new tab

[URL: https://learn.palantir.com/?utm_source=chatgpt.com]

Â©PALANTIR â€“â€“â€“â€“â€“â€“â€“â€“â€“- Learn
All courses require access to Foundry. If your organization already works with Palantir, contact your administrator for access to your account. Otherwise, eligible individuals and organizations can get started by signing-up for a free Developer Tier account.
Learn by building. Get started learning Foundry with the Speedrun: Your First End-to-End Workflow course. Build a notional workflow using core Foundry apps in 60 minutes or less.
Sign up for a Palantir Learn account so you can keep building. Continue expanding your Foundry skills by jumping into one of our Training Tracks, or dive deep into specific use cases most relevant to your work.
Data Engineerâ†’
Data Engineers prepare data for reliable use in the Ontology, ensuring quality data for downstream applications, GenAI-driven workflows, and overall decision making.
Application Developerâ†’
Application Developers in Foundry use Workshop and Quiver to build user-friendly applications, unlocking data potential and aiding informed decision-making.
AI Engineerâ†’
AI Engineers use Foundry and AIP to integrate data and design AI-enhanced workflows with LLMs, solving complex business challenges.
Data Scientistâ†’
Data Scientists use Foundry's modeling tools to create, manage, and refine predictive models, perform statistical analysis, and visualize data findings.
Frontend Developerâ†’
Frontend & OSDK Developers use the Ontology SDK to build custom application frontends or backends, leveraging Foundry as a backend service to securely accelerate application development.
Data Analystâ†’
Data Analysts use Foundry for data analysis and visualization using tools like Contour and Quiver, enabling insights without coding or complex algorithms.
SD
Get introduced to multiple Foundry and AIP applications by building quick workflows
DD
Go deep on specific applications like Workshop, Pipeline Builder, and Ontology Manager
SG
Experienced users can test their skills with an open-ended Foundry project
Ready to show off your Foundry knowledge?Earn the Foundry & AIP Builder Foundations badge by completing a short quiz. Experienced users can become officially certified in a Foundry domain by taking a Foundry Certification Exam.
Explore trending courses now â†˜
15-30 mins
15 mins
DD
45-60 mins
SR
45-60 mins
SR
60-90 mins
DD
60-90 mins
SR
60-90 mins
NP
1-2 days
Want to learn with the best? Join the Palantir Developer Community to engage with fellow builders, provide product feedback to Palantir engineers, showcase your latest work, and stay up-to-date with the latest developer content and event schedule. â†˜
â–® Start Exploring
Download reference use cases, tutorials, and starter packs directly to your Foundry enrollment.
Explore Palantirâ€™s written documentation on workflows, applications, APIs and more.
Enhance your learning with our curated YouTube channel featuring video tutorials.

[URL: https://learn.palantir.com/foundry-application-developer-associate-quiz?utm_source=chatgpt.com]

Already registered?
          
        

Sign In
rate limit
Code not recognized.
About this course
Curriculum60 Min

[URL: https://community.palantir.com/t/mock-exams-for-palantir-data-engineer-certification/4776?utm_source=chatgpt.com]

Hey,
Is there any availability of test papers to practice for the exam to get certified in Palantir Data Engineering?
Would be glad to receive any guidance on how could I best prep for the test. Iâ€™ve just looked at the guide and have been studying the documentation and youtube videos.
Best,
Siddharth
Is there a way to get certified without Taking Ontology â€˜Officialâ€™ training course ?, like:
Foundry Data Engineer Certification
Powered by Discourse, best viewed with JavaScript enabled

[URL: https://community.palantir.com/t/data-engineer-certification-preperation/2789?utm_source=chatgpt.com]

If you could share any pointers regarding Data Engineer Certification preparation.
I have seen the guide https://learn.palantir.com/data-engineer-guide-2023/1388785
But, was thinking if you have any materials, best practices and mock up tests
Hi @bedideanil - thank you for reaching out! The Exam Guide is currently all the preparation materials that we share publicly. I would also recommend looking through  topics covered on the Data Engineer Training Track. We also generally recommend users have a few months of independent work building on the platform in order to best prepare them for the exams; however, experience needs vary per user.
Hope this helps!
Garrett
This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.
Powered by Discourse, best viewed with JavaScript enabled

[URL: https://learn.palantir.com/page/training-track-data-engineer?utm_source=chatgpt.com]

Data Engineers are responsible for making data assets useful, accessible, and reliable. They own ingesting data from various systems and transforming, cleaning, and enriching data so that it can be used reliably in the Ontology. The quality of downstream decisions, Ontologies, applications, GenAI-driven workflows relies on the work Data Engineers own in preparing data.
Use the resources below for an introduction to core Data Engineer concepts. The items tagged Example are reference workflows you can deploy to your Foundry instance.

[URL: https://learn.palantir.com/app-dev-guide-2023?utm_source=chatgpt.com]

This lesson is a PDF lesson.
Open in new tab

[URL: https://learn.palantir.com/page/exam-guides?utm_source=chatgpt.com]

We have updated the Application Developer and Data Engineer Certifications to cover topics, questions, and screenshots relevant to the platform as of May 6, 2024.
Select an exam guide below or register for a Certification Exam.

[URL: https://learn.palantir.com/page/training-track-application-developer?utm_source=chatgpt.com]

Application Developers in Foundry specialize in creating operational applications on top of the Ontology using tools like Workshop and Quiver. By building efficient and user-friendly applications, Application Developers play a crucial role in helping organizations unlock the full potential of their organizationâ€™s data in the Ontology and drive informed decision-making.
The trainings and reference material below will enable you to gain hands-on experience in creating Ontology entities, designing Workshop applications, and applying write-backs using Actions - all core skills for Application Developers in Foundry.
Use the resources below for an introduction to core Application Developer concepts. The items tagged Example are reference workflows you can deploy to your Foundry instance.

[URL: https://learn.palantir.com/data-engineer-guide-2023?utm_source=chatgpt.com]

This lesson is a PDF lesson.
Open in new tab

[URL: https://learn.palantir.com/?utm_source=chatgpt.com]

Â©PALANTIR â€“â€“â€“â€“â€“â€“â€“â€“â€“- Learn
All courses require access to Foundry. If your organization already works with Palantir, contact your administrator for access to your account. Otherwise, eligible individuals and organizations can get started by signing-up for a free Developer Tier account.
Learn by building. Get started learning Foundry with the Speedrun: Your First End-to-End Workflow course. Build a notional workflow using core Foundry apps in 60 minutes or less.
Sign up for a Palantir Learn account so you can keep building. Continue expanding your Foundry skills by jumping into one of our Training Tracks, or dive deep into specific use cases most relevant to your work.
Data Engineerâ†’
Data Engineers prepare data for reliable use in the Ontology, ensuring quality data for downstream applications, GenAI-driven workflows, and overall decision making.
Application Developerâ†’
Application Developers in Foundry use Workshop and Quiver to build user-friendly applications, unlocking data potential and aiding informed decision-making.
AI Engineerâ†’
AI Engineers use Foundry and AIP to integrate data and design AI-enhanced workflows with LLMs, solving complex business challenges.
Data Scientistâ†’
Data Scientists use Foundry's modeling tools to create, manage, and refine predictive models, perform statistical analysis, and visualize data findings.
Frontend Developerâ†’
Frontend & OSDK Developers use the Ontology SDK to build custom application frontends or backends, leveraging Foundry as a backend service to securely accelerate application development.
Data Analystâ†’
Data Analysts use Foundry for data analysis and visualization using tools like Contour and Quiver, enabling insights without coding or complex algorithms.
SD
Get introduced to multiple Foundry and AIP applications by building quick workflows
DD
Go deep on specific applications like Workshop, Pipeline Builder, and Ontology Manager
SG
Experienced users can test their skills with an open-ended Foundry project
Ready to show off your Foundry knowledge?Earn the Foundry & AIP Builder Foundations badge by completing a short quiz. Experienced users can become officially certified in a Foundry domain by taking a Foundry Certification Exam.
Explore trending courses now â†˜
15-30 mins
15 mins
DD
45-60 mins
SR
45-60 mins
SR
60-90 mins
DD
60-90 mins
SR
60-90 mins
NP
1-2 days
Want to learn with the best? Join the Palantir Developer Community to engage with fellow builders, provide product feedback to Palantir engineers, showcase your latest work, and stay up-to-date with the latest developer content and event schedule. â†˜
â–® Start Exploring
Download reference use cases, tutorials, and starter packs directly to your Foundry enrollment.
Explore Palantirâ€™s written documentation on workflows, applications, APIs and more.
Enhance your learning with our curated YouTube channel featuring video tutorials.

[URL: https://learn.palantir.com/foundry-application-developer-associate-quiz?utm_source=chatgpt.com]

Already registered?
          
        

Sign In
rate limit
Code not recognized.
About this course
Curriculum60 Min

[URL: https://community.palantir.com/t/mock-exams-for-palantir-data-engineer-certification/4776?utm_source=chatgpt.com]

Hey,
Is there any availability of test papers to practice for the exam to get certified in Palantir Data Engineering?
Would be glad to receive any guidance on how could I best prep for the test. Iâ€™ve just looked at the guide and have been studying the documentation and youtube videos.
Best,
Siddharth
Is there a way to get certified without Taking Ontology â€˜Officialâ€™ training course ?, like:
Foundry Data Engineer Certification
Powered by Discourse, best viewed with JavaScript enabled

[URL: https://community.palantir.com/t/data-engineer-certification-preperation/2789?utm_source=chatgpt.com]

If you could share any pointers regarding Data Engineer Certification preparation.
I have seen the guide https://learn.palantir.com/data-engineer-guide-2023/1388785
But, was thinking if you have any materials, best practices and mock up tests
Hi @bedideanil - thank you for reaching out! The Exam Guide is currently all the preparation materials that we share publicly. I would also recommend looking through  topics covered on the Data Engineer Training Track. We also generally recommend users have a few months of independent work building on the platform in order to best prepare them for the exams; however, experience needs vary per user.
Hope this helps!
Garrett
This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.
Powered by Discourse, best viewed with JavaScript enabled

Palantir Data Engineering Certification
Volume 4 â€” Data Quality, Governance, Security & Lineage
Chapter 19 â€” Why Governance Is Foundational in Foundry
In Palantir Foundry, governance is not an optional layer added after pipelines are built. It is a
foundational design concern that shapes how data is ingested, transformed, exposed, and
operated. Foundry was designed for environments where data errors can result in regulatory
violations, financial loss, or operational failure. As a result, governance is enforced by the
platform architecture itself rather than by policy documents or manual review processes.
Traditional data platforms often rely on organizational discipline to enforce governance. Foundry
explicitly rejects this model. Instead, it encodes governance directly into dataset immutability,
lineage tracking, access controls, and ontology-driven consumption. This ensures that safe
behavior is the default and unsafe behavior is structurally difficult.


Chapter 20 â€” Data Quality as an Engineering Guarantee
In Foundry, data quality is not treated as a reporting metric or a downstream concern. It is an
engineering guarantee that must be deliberately designed into data pipelines. High-quality data
is data that can be safely used to make decisions, audited after the fact, and reproduced under
scrutiny.
Data quality must be evaluated relative to business risk. Not all datasets require the same level
of strictness. However, every curated dataset must have explicitly defined expectations around
correctness, completeness, consistency, and timeliness. These expectations must be encoded
directly into transformation logic rather than assumed.
Raw vs Curated Quality Expectations
Raw datasets exist to preserve evidence and therefore should apply minimal quality
enforcement. They may contain nulls, invalid values, duplicates, and schema inconsistencies.
Curated datasets, by contrast, represent business truth and must aggressively enforce quality
constraints. This separation ensures that quality failures are visible and traceable rather than
silently corrected.


Chapter 21 â€” Validation Strategies and Failure Modes
Validation strategies in Foundry must be chosen deliberately. Engineers must decide whether a
quality failure should halt downstream processing or merely surface a warning. This decision
depends on the risk associated with incorrect data propagation.
Fail-fast strategies are appropriate when incorrect data would lead to unsafe decisions, such as
financial reporting, regulatory submissions, or automated operational actions. Fail-soft strategies
may be acceptable for exploratory analytics or early-stage ingestion where availability is
prioritized.


Chapter 22 â€” Access Control and Security Model
Foundry enforces access control at multiple layers to ensure least-privilege access.
Dataset-level permissions control who can read or write data. Column-level controls restrict
access to sensitive fields such as personally identifiable information. Row-level security ensures
users see only the records they are authorized to view.
Security controls are declarative and centrally enforced. Engineers should never encode access
restrictions directly into transformation logic, as doing so obscures intent and complicates
auditing. Instead, security policies should be applied through Foundryâ€™s permission model and
ontology.


Chapter 23 â€” Ontology-Driven Governance
The ontology layer provides a powerful governance boundary by abstracting datasets into
business objects and actions. Users interact with entities rather than tables, which dramatically
reduces the risk of misinterpretation or misuse. Ontology-based permissions allow organizations
to expose data safely to non-technical users without sacrificing control.


Chapter 24 â€” Lineage, Auditability, and Reproducibility
Lineage in Foundry records the complete history of how every dataset was produced, including
upstream inputs, transformation logic, and execution context. This enables engineers to perform
impact analysis, reproduce historical results, and explain discrepancies long after they occur.
Reproducibility is a direct consequence of immutability and lineage. Because datasets are never
mutated in place, any historical output can be recomputed exactly. This capability is essential for
audits, debugging, and long-term trust in the data platform.


Chapter 25 â€” Common Governance Anti-Patterns
Common governance failures include exposing raw datasets to business users, embedding
security logic in transformation code, and relying on documentation rather than enforcement.
These patterns may appear expedient but undermine the safety guarantees that Foundry is
designed to provide.
The Palantir certification exam frequently tests recognition of these anti-patterns. Correct
answers consistently favor explicit governance, layered controls, and conservative design
choices.


Here's  the  complete  textbook  content  in  a  clean,  printable  format:   ---   #  **MASTERING  DATA  ENGINEERING  IN  PALANTIR  FOUNDRY**  ##  **From  Novice  to  Certified  Expert**  ###  **Comprehensive  Edition  2.0**   ---   ##  **TABLE  OF  CONTENTS**   ###  **PART  0:  FOUNDRY  MINDSET**  1.  What  Makes  Foundry  Different?  2.  Foundry's  Core  Metaphors   ###  **PART  I:  FOUNDATIONAL  CONCEPTS  (DEEP  DIVE)**  3.  Your  First  Day  in  Foundry  -  Understanding  the  Interface  4.  Foundry's  Core  Architecture  -  What's  Happening  Under  the  Hood  5.  Data  Modeling  Fundamentals  in  Foundry   ###  **PART  II:  HANDS-ON  DATA  ENGINEERING  WORKFLOWS**  6.  Data  Ingestion  -  Getting  Data  Into  Foundry  7.  Transformation  -  The  Heart  of  Data  Engineering  8.  Orchestration  -  Making  It  All  Run  Automatically  9.  Ontology  -  The  Secret  Sauce  of  Foundry   ###  **PART  III:  PRODUCTION  READINESS  &  BEST  PRACTICES**  10.  DataOps  in  Foundry  11.  Monitoring,  Alerting,  and  Observability  12.  Performance  Optimization   ###  **PART  IV:  EXAM  PREPARATION  &  CERTIFICATION**  13.  Exam  Structure  and  Topics  14.  Practice  Questions  15.  Study  Plan  16.  Exam  Day  Tips   ###  **APPENDICES**  A.  Foundry  CLI  Commands  Quick  Reference  B.  Common  PySpark  Patterns  C.  Foundry-Specific  Error  Messages  D.  Glossary  of  Foundry  Terms   ---   ##  **PART  0:  BEFORE  WE  BEGIN  -  UNDERSTANDING  FOUNDRY'S  MINDSET**   

###  **CHAPTER  0.1:  WHAT  MAKES  FOUNDRY  DIFFERENT?**   **Traditional  Data  Platforms:**  ```  Source  â†’  ETL  Pipeline  â†’  Data  Warehouse  â†’  BI  Tools  ```   **Foundry's  Approach:**  ```  Sources  â†’  Foundry  (Raw  Data  â†’  Cleaned  Data  â†’  **Ontology**)  â†’  EVERYTHING  ```   **Key  Insight:**  In  Foundry,  everything  connects  back  to  central  **business  meaning**.  
You're
 
not
 
just
 
moving
 
data;
 
you're
 
building
 
a
 
**digital
 
twin**
 
of
 
your
 
organization's
 
operations.
  ###  **CHAPTER  0.2:  FOUNDRY'S  CORE  METAPHORS**   1.  **The  Repository  is  a  Time  Machine**     -  Every  change  saved  forever  with  timestamp  and  version     -  Can  revert  to  any  point  in  time     -  Complete  audit  trail   2.  **The  Ontology  is  a  Dictionary**     -  Defines  business  terms,  not  just  technical  terms     -  Creates  common  language  across  organization     -  Enables  semantic  queries   3.  **Transforms  are  Recipes**     -  Take  ingredients  (input  data)     -  Produce  dishes  (output  data)     -  Leave  perfect  records  of  what  they  did   4.  **Workflows  are  Assembly  Lines**     -  Orchestrate  recipes  in  right  order     -  Schedule  at  right  time     -  Manage  dependencies  automatically   ---   ##  **PART  I:  FOUNDATIONAL  CONCEPTS  (DEEP  DIVE)**   ###  **CHAPTER  1:  YOUR  FIRST  DAY  IN  FOUNDRY  -  UNDERSTANDING  THE  
INTERFACE**
  ####  **1.1  THE  MAIN  APPLICATIONS**   |  Application  |  Purpose  |  Key  Tools  |  Metaphor  |  

|-------------|---------|-----------|----------|  |  **Data  Integration**  |  Bring  data  in  |  Connectors,  Loaders,  Contour  |  Loading  dock  |  |  **Transform**  |  Clean  and  shape  data  |  Code  Repos,  SQL  Transforms,  Prepare  |  Kitchen  |  |  **Orchestration**  |  Schedule  pipelines  |  Workflows,  Jobs  |  Project  manager  |  |  **Ontology**  |  Define  business  meaning  |  Object  Types,  Properties  |  Dictionary  department  |  |  **Monitor**  |  Observe  health  |  Alerts,  Metrics,  Lineage  |  Control  room  |   ####  **1.2  NAVIGATION  BASICS**   **Resource  Identifier  (RID):**  ```  ri.foundry.main.dataset.3f4b5c6d-1234-5678-9abc-def012345678  ```  -  **ri**  =  Resource  Identifier  -  **foundry.main.dataset**  =  Type  of  resource  -  **UUID**  =  Unique  identifier   **Paths  vs  RIDs:**  -  **Paths:**  Human-readable  (`/Global  Sales/Customers/USA`)  -  **RIDs:**  Permanent  identifiers  (never  change)  -  **Best  Practice:**  Use  RIDs  in  code,  paths  in  configuration   ####  **1.3  THE  FOUNDRY  FILE  SYSTEM**   ```  /  (Root)  â”œâ”€â”€  Global  (Shared  with  everyone)  â”‚    â”œâ”€â”€  Sales  â”‚    â”œâ”€â”€  Marketing  â”‚    â””â”€â”€  Operations  â”œâ”€â”€  Team  (Shared  with  your  team)  â””â”€â”€  Private  (Only  you)  ```   **Permission  Inheritance:**  -  Folders  control  visibility  -  `/Global/Sales`  =  Everyone  in  Sales  can  access  -  Child  resources  inherit  parent  permissions   ###  **CHAPTER  2:  FOUNDRY'S  CORE  ARCHITECTURE**   ####  **2.1  THREE-LAYER  ARCHITECTURE**   ```  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚          USER  INTERFACES                  â”‚  â”‚   â€¢  Web  UI     â€¢  APIs                      â”‚  â”‚   â€¢  SDKs       â€¢  SQL  Queries               â”‚  

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        FOUNDRY  SERVICES  LAYER             â”‚  â”‚   â€¢  Ontology  Service                      â”‚  â”‚   â€¢  Transform  Service                     â”‚  â”‚   â€¢  Workflow  Service                      â”‚  â”‚   â€¢  Catalog  Service                       â”‚  â”‚   â€¢  Security  Service                      â”‚  â”‚   â€¢  Lineage  Service                       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚      COMPUTE  &  STORAGE  LAYER              â”‚  â”‚   â€¢  Apache  Spark  Clusters                 â”‚  â”‚   â€¢  Foundry  Functions                     â”‚  â”‚   â€¢  PostgreSQL/Delta  Lake                 â”‚  â”‚   â€¢  Object  Storage                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  ```   ####  **2.2  DATA  STORAGE  -  DATASETS**   **Dataset  Structure:**  ```  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  METADATA                             â”‚  â”‚  â€¢  Name,  Description                  â”‚  â”‚  â€¢  Schema,  Owner                      â”‚  â”‚  â€¢  Created/Modified  dates             â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚  DATA                                 â”‚  â”‚  â€¢  Stored  as  Parquet/Delta  files      â”‚  â”‚  â€¢  Partitioned  for  performance        â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚  HISTORY                              â”‚  â”‚  â€¢  Every  change  =  new  snapshot        â”‚  â”‚  â€¢  Full  version  history               â”‚  â”‚  â€¢  Rollback  capability                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  ```   **Dataset  Types:**  1.  **Spark  Datasets**     -  Built  on  Apache  Spark     -  Terabyte-scale  processing     -  Parquet/Delta  storage   

2.  **SQL  Datasets**     -  Materialized  SQL  views     -  Auto-refresh  on  source  change     -  Great  for  business  logic   3.  **Ontology  Datasets**     -  Directly  tied  to  Object  Types     -  Auto-populated  on  sync     -  Enable  semantic  queries   ####  **2.3  VERSIONING  IN  ACTION**   **Monday:**  -  Dataset  RID:  `ri.dataset.aaa`  -  Contains:  Customer  123  =  "John  Smith"   **Tuesday  (after  update):**  -  New  RID:  `ri.dataset.bbb`  -  Contains:  Customer  123  =  "John  Q.  Smith"  -  `ri.dataset.aaa`  still  exists  unchanged  -  Lineage:  `aaa  â†’  bbb`   **Key  Benefit:**  Reproducibility.  Pipelines  using  specific  RIDs  always  get  same  data.   ###  **CHAPTER  3:  DATA  MODELING  FUNDAMENTALS**   ####  **3.1  DATA  HIERARCHY**   ```  Level  0:  Files  (CSV,  JSON,  PDF)       â†“  Loaders/Contour  Level  1:  Raw  Datasets  (As-is  from  source)       â†“  Transform  Level  2:  Cleaned  Datasets  (Business-ready)       â†“  Apply  Links  Level  3:  Ontology  Objects  (Business  meaning)       â†“  Ontology  Sync  Level  4:  Connected  Knowledge  Graph  ```   ####  **3.2  SCHEMA  MANAGEMENT**   **Schema-on-Read  (Flexible):**  ```python  #  Foundry  infers  schema  automatically  df  =  spark.read.csv("/path/to/file")  #  Can  handle  new  columns  ```  

 **Schema  Enforcement  (Strict):**  ```python  from  pyspark.sql.types  import  StructType,  StringType,  IntegerType   schema  =  StructType([      StructField("customer_id",  IntegerType(),  True),      StructField("name",  StringType(),  False)   #  Required  field  ])  ```   **Best  Practice  Progression:**  1.  Exploration:  Schema-on-read  2.  Development:  Add  validation  3.  Production:  Enforce  strict  schema   ---   ##  **PART  II:  HANDS-ON  DATA  ENGINEERING  WORKFLOWS**   ###  **CHAPTER  4:  DATA  INGESTION**   ####  **4.1  THREE  INGESTION  METHODS**   |  Method  |  When  to  Use  |  Frequency  |  Complexity  |  |--------|-------------|-----------|------------|  |  **Connectors  &  Loaders**  |  Scheduled  imports  |  Scheduled  |  Medium  |  |  **Contour**  |  One-time,  exploratory  |  Manual  |  Low  |  |  **API  Upload**  |  Programmatic  needs  |  Event-driven  |  High  |   ####  **4.2  LOADER  CONFIGURATION  EXAMPLE**   ```yaml  #  salesforce_loader.yaml  source:    type:  salesforce    connection:  salesforce-prod    object:  Opportunity    query:  |      SELECT  Id,  Name,  Amount,  CloseDate      FROM  Opportunity      WHERE  LastModifiedDate  >=  YESTERDAY   destination:    path:  /Global/Sales/bronze/opportunities    format:  parquet    mode:  append     

schedule:    cron:  "0  3  *  *  *"   #  3  AM  daily    timezone:  UTC     notifications:    on_failure:      -  email:  data-team@company.com      -  slack:  "#data-alerts"  ```   ####  **4.3  BRONZE-SILVER-GOLD  PATTERN**   **Bronze  Layer  (Raw):**  -  Path:  `/project/raw/`  -  Strategy:  Append-only  -  No  transformations  -  Preserve  source  fidelity   **Silver  Layer  (Cleaned):**  -  Path:  `/project/cleaned/`  -  Actions:  Deduplicate,  validate,  standardize  -  Add:  Surrogate  keys,  business  rules   **Gold  Layer  (Business):**  -  Path:  `/project/gold/`  -  Contains:  Aggregates,  joined  views  -  Ready  for:  Analytics,  reporting,  ML   ###  **CHAPTER  5:  TRANSFORMATION**   ####  **5.1  TOOL  SELECTION  MATRIX**   |  Tool  |  Best  For  |  Scale  |  Skill  Required  |  |------|----------|-------|----------------|  |  **Prepare**  |  Simple  cleansing  |  Small  |  Low  (no-code)  |  |  **SQL  Transform**  |  Business  logic  |  Medium  |  Medium  (SQL)  |  |  **PySpark  Transform**  |  Complex  processing  |  Large  |  High  (Python)  |  |  **Foundry  Functions**  |  Event-driven  tasks  |  Micro  |  Medium  (Python/TS)  |   ####  **5.2  CODE  REPOSITORY  STRUCTURE**   ```  /customer-pipeline/  â”œâ”€â”€  transforms-python/  â”‚    â”œâ”€â”€  __init__.py  â”‚    â”œâ”€â”€  clean_customers.py  â”‚    â””â”€â”€  enrich_orders.py  â”œâ”€â”€  transforms-spark/  

â”‚    â””â”€â”€  process_large_data.scala  â”œâ”€â”€  tests/  â”‚    â”œâ”€â”€  test_clean_customers.py  â”‚    â””â”€â”€  test_data_quality.py  â”œâ”€â”€  .pre-commit-config.yaml  â”œâ”€â”€  .synthea-build.yaml  â””â”€â”€  README.md  ```   ####  **5.3  PRODUCTION  PYSPARK  TRANSFORM**   ```python  """  CLEAN  CUSTOMER  DATA  TRANSFORM  Input:  Raw  customer  data  Output:  Cleaned  customer  data  +  Quality  report  """   from  transforms.api  import  transform,  Input,  Output  import  pyspark.sql.functions  as  F  from  pyspark.sql.window  import  Window   @transform(      raw_customers=Input("/Global/Sales/bronze/customers"),      cleaned_customers=Output("/Global/Sales/silver/customers_clean"),      quality_report=Output("/Global/Sales/reports/quality_daily")  )  def  clean_customer_data(raw_customers,  cleaned_customers,  quality_report):           #  1.  READ  RAW  DATA      df  =  raw_customers.dataframe()           #  2.  BASIC  CLEANING      #  Trim  whitespace      for  col_name,  col_type  in  df.dtypes:          if  col_type  ==  "string":              df  =  df.withColumn(col_name,  F.trim(F.col(col_name)))           #  3.  STANDARDIZE  COUNTRY  CODES      country_map  =  {          "United  States":  "USA",          "US":  "USA",          "United  Kingdom":  "UK",          "GB":  "UK"      }           mapping_expr  =  F.create_map([F.lit(x)  for  pair  in  country_map.items()  for  x  in  pair])      df  =  df.withColumn("country_std",   

                      F.coalesce(mapping_expr[F.col("country")],                                   F.col("country")))           #  4.  DEDUPLICATE  (KEEP  MOST  RECENT)      window  =  Window.partitionBy("customer_id").orderBy(F.col("updated_at").desc())      df  =  (df.withColumn("row_num",  F.row_number().over(window))             .filter(F.col("row_num")  ==  1)             .drop("row_num"))           #  5.  QUALITY  CHECKS      checks  =  []           #  Uniqueness  check      unique_ratio  =  df.select("customer_id").distinct().count()  /  df.count()      checks.append({          "check":  "customer_id_uniqueness",          "passed":  unique_ratio  >=  0.99,          "value":  unique_ratio      })           #  Null  check      for  field  in  ["customer_id",  "email"]:          null_pct  =  df.filter(F.col(field).isNull()).count()  /  df.count()          checks.append({              "check":  f"{field}_not_null",              "passed":  null_pct  <=  0.01,              "value":  null_pct          })           #  6.  CREATE  QUALITY  REPORT      df_report  =  spark.createDataFrame(checks)           #  7.  WRITE  OUTPUTS      cleaned_customers.write_dataframe(          df,          partition_cols=["country_std",  "load_date"]      )           quality_report.write_dataframe(df_report)           #  8.  LOG  METRICS      cleaned_customers.set_stat("row_count",  df.count())      cleaned_customers.set_stat("quality_passed",  all(c["passed"]  for  c  in  checks))  ```   ####  **5.4  SQL  TRANSFORM  EXAMPLE**   ```sql  

--  customer_360_view.sql  --  Business-ready  customer  360  view   WITH  customer_orders  AS  (      SELECT          customer_id,          COUNT(*)  as  total_orders,          SUM(amount)  as  lifetime_value,          MAX(order_date)  as  last_order_date      FROM  `/Global/Sales/silver/orders`      WHERE  status  =  'COMPLETED'      GROUP  BY  customer_id  ),   customer_support  AS  (      SELECT          customer_id,          COUNT(*)  as  ticket_count,          SUM(CASE  WHEN  status  =  'OPEN'  THEN  1  ELSE  0  END)  as  open_tickets      FROM  `/Global/Support/tickets`      GROUP  BY  customer_id  )   SELECT      c.*,      COALESCE(co.total_orders,  0)  as  total_orders,      COALESCE(co.lifetime_value,  0)  as  lifetime_value,      co.last_order_date,      COALESCE(cs.ticket_count,  0)  as  ticket_count,      COALESCE(cs.open_tickets,  0)  as  open_tickets,           --  Customer  health  score      CASE          WHEN  co.lifetime_value  >  10000  AND  cs.open_tickets  =  0  THEN  'HEALTHY'          WHEN  co.lifetime_value  <  1000  OR  cs.open_tickets  >  3  THEN  'RISK'          ELSE  'NEUTRAL'      END  as  health_score,           CURRENT_TIMESTAMP()  as  snapshot_time       FROM  `/Global/Sales/silver/customers_clean`  c  LEFT  JOIN  customer_orders  co  ON  c.customer_id  =  co.customer_id  LEFT  JOIN  customer_support  cs  ON  c.customer_id  =  cs.customer_id  ```   ###  **CHAPTER  6:  ORCHESTRATION**   ####  **6.1  WORKFLOW  DAG  EXAMPLE**  

 ```                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚    Start  (4  AM)   â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â–¼                          â–¼          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚  Load  Customersâ”‚          â”‚  Load  Orders   â”‚          â”‚   (Job  A)       â”‚          â”‚   (Job  B)      â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚                          â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚  Create  360  Viewâ”‚                      â”‚     (Job  C)      â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚  Apply  Ontology  â”‚                      â”‚     (Job  D)      â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚  Send  Daily      â”‚                      â”‚    Report        â”‚                      â”‚     (Job  E)      â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  ```   ####  **6.2  WORKFLOW  DEFINITION**   ```yaml  #  daily-customer-pipeline.yaml  name:  "daily-customer-pipeline"  description:  "Process  customer  data  daily"   schedule:    trigger:  "cron"    expression:  "0  4  *  *  *"   #  4  AM  daily    timezone:  "America/New_York"   jobs:    load_raw_customers:      type:  spark      transform:  "/transforms/load_customers"  

    resources:        executor_instances:  4        executor_memory:  "8g"           clean_customer_data:      type:  spark      transform:  "/transforms/clean_customers"      depends_on:  ["load_raw_customers"]         create_customer_360:      type:  sql      query:  "/queries/customer_360.sql"      depends_on:  ["clean_customer_data"]         send_daily_report:      type:  email      to:  "sales-team@company.com"      subject:  "Daily  Customer  Update"      body:  "Processed  {{clean_customer_data.output_row_count}}  customers"      depends_on:  ["create_customer_360"]       notifications:    on_failure:      -  type:  slack        channel:  "#data-alerts"        message:  "Pipeline  failed:  {{workflow.error_message}}"  ```   ####  **6.3  ADVANCED  PATTERNS**   **Pattern  1:  Fan-Out,  Fan-In**  ```yaml  #  Process  regions  in  parallel  process_usa:    type:  spark    transform:  "/transforms/process_region"    parameters:  {"region":  "USA"}     process_europe:    type:  spark    transform:  "/transforms/process_region"    parameters:  {"region":  "EUROPE"}     process_asia:    type:  spark    transform:  "/transforms/process_region"    parameters:  {"region":  "ASIA"}     

combine_results:    type:  spark    transform:  "/transforms/combine_regions"    depends_on:  ["process_usa",  "process_europe",  "process_asia"]  ```   **Pattern  2:  Conditional  Execution**  ```yaml  check_data_quality:    type:  spark    transform:  "/transforms/quality_check"     process_data:    type:  spark    transform:  "/transforms/process"    condition:  "{{check_data_quality.result.passed}}"    depends_on:  ["check_data_quality"]     alert_on_failure:    type:  email    condition:  "not  {{check_data_quality.result.passed}}"    depends_on:  ["check_data_quality"]  ```   ###  **CHAPTER  7:  ONTOLOGY**   ####  **7.1  OBJECT  TYPE  DEFINITION**   ```yaml  #  customer_object.yaml  object_type:    name:  "Customer"    description:  "Company  purchasing  our  products"       primary_key:      -  "customer_id"       properties:      -  name:  "customer_id"        type:  "string"        required:  true             -  name:  "customer_name"        type:  "string"             -  name:  "industry"        type:  "enum"        allowed_values:  ["TECH",  "FINANCE",  "HEALTHCARE",  "RETAIL"]  

           -  name:  "annual_revenue"        type:  "decimal"        unit:  "USD"             -  name:  "relationship_manager"        type:  "link"        linked_object_type:  "Employee"             -  name:  "contracts"        type:  "link"        linked_object_type:  "Contract"        cardinality:  "many"  ```   ####  **7.2  APPLYING  LINKS**   ```python  from  transforms.api  import  transform,  Input,  Output  from  transforms.ontology  import  apply_links   @transform(      customers=Input("/Global/Sales/silver/customers_clean"),      customers_linked=Output("/Global/Sales/gold/customers_linked")  )  def  link_customers(customers,  customers_linked):      df  =  customers.dataframe()           linked_df  =  apply_links(          dataframe=df,          links={              "customer_id":  "Customer.customer_id",              "company_name":  "Customer.customer_name",              "sales_rep_id":  "Customer.relationship_manager",              "industry_code":  "Customer.industry"          }      )           customers_linked.write_dataframe(linked_df)  ```   ####  **7.3  ONTOLOGY  QUERYING**   **Traditional  SQL  (Joins  Required):**  ```sql  SELECT       c.customer_name,      o.order_date,  

    o.amount  FROM  customers  c  JOIN  orders  o  ON  c.customer_id  =  o.customer_id  WHERE  c.country  =  'USA'  ```   **Ontology  SQL  (No  Explicit  Joins):**  ```sql  SELECT       customer.name  AS  customer_name,      customer.orders.order_date,      customer.orders.amount,      customer.contracts.start_date,      customer.relationship_manager.email  FROM  Customer  WHERE  customer.country  =  'USA'    AND  customer.industry  =  'TECH'    AND  customer.contracts.status  =  'ACTIVE'  ```   **How  It  Works:**  1.  Foundry  understands  relationships  from  ontology  2.  Automatically  generates  optimal  joins  3.  You  write  business  logic,  not  database  logic   ####  **7.4  ONTOLOGY  SYNC  PROCESS**   ```        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚    Datasets   â”‚     â”‚   Ontology    â”‚     â”‚    Graph      â”‚        â”‚    (Clean)    â”‚â”€â”€â”€ â–¶ â”‚     Links     â”‚â”€â”€â”€ â–¶ â”‚   Database    â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                 â”‚                                                 â–¼                                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚  Queryable    â”‚                                         â”‚  SQL  Views    â”‚                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  ```   **Sync  Steps:**  1.  Apply  links  in  transforms  2.  Go  to  Ontology  Manager  3.  Select  Object  Types  4.  Click  "Sync"  5.  Wait  for  completion  (minutes  to  hours)  6.  Query  using  ontology  syntax   

---   ##  **PART  III:  PRODUCTION  READINESS  &  BEST  PRACTICES**   ###  **CHAPTER  8:  DATAOPS  IN  FOUNDRY**   ####  **8.1  BRANCHING  STRATEGY**   **Development  Flow:**  ```  Main  Branch  (production)      â†‘  Cut  Staging  Branch  (UAT)      â†‘  Merge  Feature  Branch  (development)  ```   **Branch  Creation:**  ```bash  #  Create  feature  branch  foundry  branch  create  --name  "feature/add-segmentation"   #  Work  in  branch  /path/in/branch/feature/add-segmentation/   #  Merge  to  staging  foundry  merge  --source  feature/add-segmentation  --target  staging   #  Cut  to  production  foundry  cut  create  --source  staging  --target  main  --message  "Release  v1.2.0"  ```   ####  **8.2  CI/CD  PIPELINE**   ```yaml  #  .synthea-build.yaml  name:  Data  Pipeline  CI/CD   on:    push:      branches:  [main,  staging]    pull_request:      branches:  [main]   jobs:    test:      runs-on:  foundry-spark       

    steps:      -  name:  Checkout  code        uses:  actions/checkout@v2             -  name:  Run  unit  tests        run:  python  -m  pytest  tests/  -v             -  name:  Validate  schemas        run:  python  scripts/validate_schemas.py             -  name:  Check  data  quality        run:  python  tests/quality/test_suite.py       deploy:      needs:  test      if:  github.ref  ==  'refs/heads/main'           steps:      -  name:  Cut  to  production        run:  foundry  cut  create  --source  staging  --target  main             -  name:  Deploy  workflow        run:  foundry  workflow  deploy  pipeline.yaml             -  name:  Run  smoke  tests        run:  python  tests/smoke/test_production.py  ```   ####  **8.3  TESTING  FRAMEWORK**   **Unit  Test  Example:**  ```python  #  tests/test_customer_transform.py  import  pytest  from  transforms.api  import  Input,  Output  from  pyspark.sql  import  SparkSession  import  clean_customers   def  test_customer_cleaning(spark):      """Test  customer  data  cleaning  logic."""           #  Create  test  data      test_data  =  [          (1,  "   JOHN   ",  "US",  "john@email.com"),          (2,  "Jane",  "United  States",  "jane@email.com")      ]           test_df  =  spark.createDataFrame(  

        test_data,          ["customer_id",  "name",  "country",  "email"]      )           #  Mock  Foundry  objects      class  MockInput:          def  dataframe(self):              return  test_df           class  MockOutput:          def  write_dataframe(self,  df):              self.result  =  df           #  Execute  transform      input_mock  =  MockInput()      output_mock  =  MockOutput()           clean_customers.clean_customer_data(input_mock,  output_mock)           #  Assertions      result  =  output_mock.result      assert  result.count()  ==  2      assert  result.filter("name  =  '   JOHN   '").count()  ==  0      assert  result.filter("country  =  'USA'").count()  ==  2  ```   ###  **CHAPTER  9:  MONITORING  &  ALERTING**   ####  **9.1  MONITORING  DASHBOARD  ELEMENTS**   **Pipeline  Health:**  -  âœ…  Success  rate  (target:  >99%)  -  â±  Average  duration  (track  trends)  -  ðŸ“ˆ  Resource  utilization  -  â³  Queue  wait  times   **Data  Quality:**  -  ðŸ“Š  Row  count  changes  -  âŒ  Null  value  percentages  -  ðŸ”„  Schema  drift  detection  -  ðŸ•’  Freshness  (time  since  update)   ####  **9.2  ALERT  CONFIGURATION**   ```yaml  #  dataset_monitors.yaml  monitors:    -  type:  row_count  

    dataset:  "/Global/Sales/gold/customer_360"      condition:  "change_percentage  >  30"      action:  "alert"         -  type:  freshness      dataset:  "/Global/Sales/bronze/customers"      expected_interval:  "24h"      action:  "email:data-team@company.com"         -  type:  schema_change      dataset:  "/Global/Sales/silver/orders"      action:  "slack:#data-schema-changes"         -  type:  data_quality      dataset:  "/Global/Sales/gold/revenue"      checks:        -  column:  "revenue_amount"          rule:  ">  0"        -  column:  "customer_id"          rule:  "not_null"      action:  "pagerduty:data-engineers"  ```   ####  **9.3  LINEAGE  ANALYSIS**   **Questions  Lineage  Answers:**  1.  **Upstream:**  "Where  did  this  number  come  from?"  2.  **Downstream:**  "Who's  using  this  dataset?"  3.  **Impact:**  "What  breaks  if  I  change  this?"  4.  **Timeline:**  "How  long  does  data  flow  take?"   **Lineage  Visualization:**  ```  Dataset  A  (v1.0)  â†’  Transform  X  â†’  Dataset  B  (v1.0)         â†“                                 â†“  Dataset  A  (v2.0)  â†’  Transform  X  â†’  Dataset  B  (v2.0)         â†“                                 â†“      Source                           Dashboard                                        â†“                                   Business  User  ```   ###  **CHAPTER  10:  PERFORMANCE  OPTIMIZATION**   ####  **10.1  SPARK  OPTIMIZATION**   **Common  Issue  1:  Data  Skew**  ```python  

#  PROBLEM:  Some  keys  have  millions  of  records  df.join(large_df,  "customer_id")   #  Causes  skew   #  SOLUTION  1:  Salting  technique  df.withColumn("salt",  (F.rand()  *  100).cast("int"))    .join(large_df.withColumn("salt",  (F.rand()  *  100).cast("int")),          ["customer_id",  "salt"])     #  SOLUTION  2:  Broadcast  for  small  tables  from  pyspark.sql.functions  import  broadcast  df.join(broadcast(small_df),  "key")  ```   **Common  Issue  2:  Too  Many  Small  Files**  ```python  #  Write  with  optimal  file  size  (df.repartition(100)   #  Aim  for  ~100MB  files     .write     .parquet("/output"))   #  Enable  adaptive  query  execution  spark.conf.set("spark.sql.adaptive.enabled",  "true")  spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled",  "true")  spark.conf.set("spark.sql.adaptive.advisoryPartitionSizeInBytes",  "128MB")  ```   **Common  Issue  3:  Memory  Issues**  ```python  #  Increase  memory  for  heavy  operations  spark.conf.set("spark.executor.memory",  "16g")  spark.conf.set("spark.driver.memory",  "8g")  spark.conf.set("spark.memory.fraction",  "0.8")  ```   ####  **10.2  FOUNDRY-SPECIFIC  OPTIMIZATIONS**   **1.  Partition  Strategy:**  ```python  #  Time-based  partitioning  df.write.partitionBy("year",  "month",  "day").parquet("/output")   #  Business  key  partitioning  df.write.partitionBy("region",  "department").parquet("/output")  ```   **2.  Incremental  Processing:**  ```python  #  Process  only  new/changed  data  

last_run  =  get_last_successful_run()  new_data  =  df.filter(F.col("updated_at")  >  last_run)   #  Watermark  technique  df.withWatermark("event_time",  "1  hour")    .groupBy("customer_id",  window("event_time",  "1  hour"))    .count()  ```   **3.  Caching  Strategy:**  ```python  #  Cache  frequently  used  datasets  df.cache().count()   #  Materialize  cache   #  Check  if  caching  helps  if  df.storageLevel.useMemory:      print("Dataset  is  cached  in  memory")  ```   **4.  Query  Optimization:**  ```sql  --  Use  predicate  pushdown  SELECT  *  FROM  orders  WHERE  order_date  >=  '2024-01-01'    AND  region  =  'USA'   --  Foundry  pushes  to  storage  layer     --  Avoid  SELECT  *  SELECT  customer_id,  order_date,  amount   --  Only  needed  columns  FROM  orders   --  Use  appropriate  join  types  --  Foundry  optimizes  based  on  statistics  ```   ####  **10.3  PERFORMANCE  CHECKLIST**   **Before  Production:**  -  [  ]  Partition  columns  defined  -  [  ]  File  sizes  optimized  (~100MB  each)  -  [  ]  Data  skew  addressed  -  [  ]  Appropriate  join  strategies  -  [  ]  Memory  settings  tuned  -  [  ]  Caching  strategy  defined   **Monitoring  in  Production:**  -  [  ]  Spark  UI  metrics  tracked  -  [  ]  Duration  baselines  established  -  [  ]  Alert  thresholds  set  

-  [  ]  Resource  utilization  monitored  -  [  ]  Query  plans  reviewed  regularly   ---   ##  **PART  IV:  EXAM  PREPARATION  &  CERTIFICATION**   ###  **CHAPTER  11:  EXAM  STRUCTURE**   **Exam  Details:**  -  **Questions:**  60-80  multiple  choice  -  **Time:**  120  minutes  -  **Format:**  Online  proctored  -  **Passing  Score:**  ~70%   **Topic  Weights:**  1.  **Data  Ingestion  (20%):**  Connectors,  Loaders,  Contour  2.  **Transformation  (25%):**  Code  Repos,  SQL  Transforms,  PySpark  3.  **Orchestration  (20%):**  Workflows,  Jobs,  Scheduling  4.  **Ontology  (20%):**  Object  Types,  Links,  Querying  5.  **Operations  (15%):**  Monitoring,  Security,  Best  Practices   ###  **CHAPTER  12:  PRACTICE  QUESTIONS**   ####  **QUESTION  1:  INGESTION**  **Scenario:**  You  need  to  ingest  50  GB  of  CSV  data  from  S3  daily.  Files  arrive  at  random  
times.
 
What's
 
the
 
MOST
 
efficient
 
approach?
  **Options:**  A)  Use  Contour  to  manually  upload  each  file  B)  Create  a  Loader  with  daily  schedule  C)  Use  Foundry  Function  triggered  by  S3  events  D)  Write  Python  script  using  Foundry  API   **Answer:  B**  (Loader  with  schedule)  **Explanation:**  C  would  work  but  over-engineered.  Loaders  are  designed  for  scheduled  
bulk
 
ingestion.
  ####  **QUESTION  2:  TRANSFORMATION**  **Scenario:**  PySpark  job  failing  with  "Out  of  Memory"  on  driver.  First  action?   **Options:**  A)  Increase  spark.driver.memory  B)  Add  more  partitions  C)  Check  for  data  skew  using  Spark  UI  D)  Switch  to  SQL  transforms   **Answer:  C**  (Check  for  skew)  

**Explanation:**  Always  diagnose  before  treating.  Skew  is  common  cause  of  OOM.   ####  **QUESTION  3:  ONTOLOGY**  **Scenario:**  Dataset  has  employee_id,  manager_id,  department_id.  Want  to  query  
employees
 
and
 
navigate
 
to
 
managers.
 
First
 
step?
  **Options:**  A)  Create  SQL  views  B)  Apply  links  to  Object  Types  C)  Write  transform  to  join  tables  D)  Use  Prepare  to  merge  datasets   **Answer:  B**  (Apply  links)  **Explanation:**  Links  connect  data  to  ontology  enabling  navigation  queries.   ####  **QUESTION  4:  ORCHESTRATION**  **Scenario:**  Job  B  depends  on  Job  A.  Job  A  fails.  What  happens?   **Options:**  A)  Job  B  runs  anyway  B)  Job  B  waits  for  manual  restart  C)  Job  B  is  skipped  D)  Workflow  fails  immediately   **Answer:  C**  (Job  B  is  skipped)  **Explanation:**  Dependent  jobs  skip  when  dependencies  fail  (configurable).   ####  **QUESTION  5:  BEST  PRACTICES**  **Scenario:**  Team  of  5  engineers  working  on  same  pipeline.  Best  approach?   **Options:**  A)  All  work  in  main  branch  B)  Use  feature  branches  C)  Create  separate  projects  D)  Work  in  different  folders   **Answer:  B**  (Feature  branches)  **Explanation:**  Branches  enable  parallel  development  with  isolation.   ###  **CHAPTER  13:  6-WEEK  STUDY  PLAN**   **Week  1-2:  Foundation  Building**  -  [  ]  Complete  Foundry  tutorials  -  [  ]  Learn  basic  PySpark  -  [  ]  Practice  SQL  transforms  -  [  ]  Create  simple  pipeline   **Week  3-4:  Hands-On  Practice**  

-  [  ]  Build  bronze-silver-gold  pipeline  -  [  ]  Create  Object  Type  and  links  -  [  ]  Set  up  scheduled  workflow  -  [  ]  Implement  data  quality  checks   **Week  5:  Advanced  Topics**  -  [  ]  Study  performance  optimization  -  [  ]  Practice  monitoring  setup  -  [  ]  Review  security  models  -  [  ]  Understand  CI/CD  in  Foundry   **Week  6:  Exam  Preparation**  -  [  ]  Take  practice  exams  -  [  ]  Review  official  documentation  -  [  ]  Join  community  forums  -  [  ]  Schedule  exam   **Daily  Study  Routine:**  -  Morning  (30  min):  Review  concepts  -  Afternoon  (60  min):  Hands-on  practice  -  Evening  (30  min):  Practice  questions   ###  **CHAPTER  14:  EXAM  DAY  STRATEGY**   **Before  Exam:**  1.  **Technical  Check:**     -  Test  computer  and  internet     -  Close  all  unnecessary  applications     -  Clear  workspace  (proctoring  requirements)   2.  **Materials  Ready:**     -  Government  ID     -  Water  bottle     -  Scratch  paper  and  pen  (if  allowed)   3.  **Mental  Preparation:**     -  Review  key  concepts     -  Practice  breathing  exercises     -  Set  positive  mindset   **During  Exam:**  1.  **Time  Management:**     -  First  pass:  Answer  known  questions  (60  minutes)     -  Second  pass:  Review  flagged  questions  (40  minutes)     -  Final  check:  Review  all  answers  (20  minutes)   2.  **Question  Strategy:**     -  Read  each  question  twice  

   -  Eliminate  obviously  wrong  answers     -  Flag  uncertain  questions     -  Watch  for  "MOST"  and  "BEST"  keywords   3.  **Technical  Questions:**     -  Think  about  scalability     -  Consider  Foundry  best  practices     -  Remember  specific  Foundry  terminology   **Common  Pitfalls  to  Avoid:**  -  âŒ  Overthinking  simple  questions  -  âŒ  Changing  answers  without  reason  -  âŒ  Spending  too  long  on  one  question  -  âŒ  Forgetting  about  business  context  -  âŒ  Ignoring  Foundry-specific  features   **After  Exam:**  -  Take  notes  on  difficult  questions  -  Celebrate  completion  -  Plan  next  steps  regardless  of  outcome  -  Request  detailed  feedback  if  available   ---   ##  **APPENDICES**   ###  **APPENDIX  A:  FOUNDRY  CLI  QUICK  REFERENCE**   ```bash  #  DATASET  COMMANDS  foundry  dataset  list  --path  "/Global/Sales"  foundry  dataset  read  --rid  ri.dataset.abc  --limit  10  foundry  dataset  write  --path  "/my/dataset"  --file  data.csv  foundry  dataset  delete  --rid  ri.dataset.abc   #  TRANSFORM  COMMANDS  foundry  transform  build  --path  "/my/transform"  foundry  transform  test  --path  "/my/transform"  foundry  transform  deploy  --path  "/my/transform"   #  BRANCH  COMMANDS  foundry  branch  list  foundry  branch  create  --name  "feature/new-transform"  foundry  branch  delete  --name  "old-branch"  foundry  cut  create  --source  staging  --target  main  --message  "Release  v1.0"   #  WORKFLOW  COMMANDS  foundry  workflow  list  

foundry  workflow  run  --name  "daily-pipeline"  foundry  workflow  logs  --run-id  run-123  foundry  workflow  status  --name  "daily-pipeline"   #  AUTHENTICATION  foundry  login  foundry  logout  foundry  whoami   #  PROJECT  MANAGEMENT  foundry  project  create  --name  "Sales-Analytics"  foundry  project  list  foundry  project  info  --name  "Sales-Analytics"  ```   ###  **APPENDIX  B:  COMMON  PYSPARK  PATTERNS**   ```python  #  1.  READING  DATA  df  =  spark.read.parquet("/path/to/dataset")  df  =  spark.read.csv("/path/to/csv",  header=True,  inferSchema=True)  df  =  spark.read.json("/path/to/json")   #  2.  WRITING  DATA  (df.write     .mode("overwrite")   #  or  "append",  "ignore",  "error"     .partitionBy("date")     .parquet("/output/path"))   #  3.  COMMON  TRANSFORMATIONS  #  Filtering  df  =  df.filter(F.col("status")  ==  "ACTIVE")   #  Adding  columns  df  =  df.withColumn("full_name",                      F.concat(F.col("first_name"),                              F.lit("  "),                              F.col("last_name")))   #  Aggregations  df_agg  =  (df.groupBy("department")             .agg(F.count("*").alias("employee_count"),                  F.avg("salary").alias("avg_salary"))             .orderBy(F.desc("employee_count")))   #  Window  functions  from  pyspark.sql.window  import  Window  window_spec  =  Window.partitionBy("department").orderBy("salary")  

df  =  df.withColumn("salary_rank",  F.row_number().over(window_spec))   #  Handling  nulls  df  =  df.fillna({"department":  "Unknown",  "salary":  0})  df  =  df.dropna(subset=["employee_id",  "email"])   #  Type  casting  df  =  df.withColumn("salary",  F.col("salary").cast("decimal(10,2)"))  ```   ###  **APPENDIX  C:  ERROR  MESSAGES  &  SOLUTIONS**   |  Error  Message  |  Likely  Cause  |  Solution  |  |--------------|--------------|----------|  |  `Resource  not  found`  |  Incorrect  RID  or  path  |  Verify  spelling,  check  permissions  |  |  `Permission  denied`  |  Insufficient  folder  access  |  Request  access,  check  parent  folder  
permissions
 
|
 |  `Transform  build  failed`  |  Syntax  error  in  code  |  Check  Python/Scala  syntax,  dependencies  |  |  `Job  timeout`  |  Job  running  too  long  |  Increase  timeout,  optimize  code,  check  for  infinite  
loops
 
|
 |  `Out  of  memory`  |  Data  skew  or  insufficient  memory  |  Check  Spark  UI  for  skew,  increase  
executor
 
memory
 
|
 |  `Connection  refused`  |  Network  or  service  issue  |  Check  Foundry  status  page,  verify  
network
 
connectivity
 
|
 |  `Invalid  credentials`  |  Authentication  expired  |  Run  `foundry  login`  to  refresh  |  |  `Dataset  schema  mismatch`  |  Schema  changed  unexpectedly  |  Check  upstream  changes,  
enforce
 
schema
 
validation
 
|
 |  `Partition  column  not  found`  |  Wrong  column  name  in  partitionBy  |  Verify  column  exists,  
check
 
case
 
sensitivity
 
|
 |  `Duplicate  output  dataset`  |  Multiple  writes  to  same  path  |  Ensure  unique  output  paths,  
check
 
for
 
race
 
conditions
 
|
  ###  **APPENDIX  D:  GLOSSARY  OF  FOUNDRY  TERMS**   |  Term  |  Definition  |  |------|-----------|  |  **RID**  |  Resource  Identifier  -  unique  ID  for  everything  in  Foundry  |  |  **Dataset**  |  Table-like  structure  storing  data  in  Foundry  |  |  **Transform**  |  Code  that  processes  data  from  inputs  to  outputs  |  |  **Workflow**  |  DAG  of  jobs  that  run  on  schedule  or  trigger  |  |  **Job**  |  Single  unit  of  work  in  a  workflow  |  |  **Ontology**  |  Graph-based  model  of  business  concepts  and  relationships  |  |  **Object  Type**  |  Blueprint  for  business  entities  in  ontology  |  |  **Link**  |  Connection  between  dataset  column  and  ontology  property  |  |  **Branch**  |  Isolated  workspace  for  development  |  |  **Cut**  |  Process  of  promoting  changes  between  branches  |  |  **Loader**  |  Configuration  for  scheduled  data  ingestion  |  |  **Connector**  |  Pre-built  adapter  for  external  data  sources  |  

|  **Contour**  |  UI  tool  for  manual  data  upload  and  exploration  |  |  **Lineage**  |  Tracking  of  data  flow  and  dependencies  |  |  **Monitor**  |  Automated  check  for  data  quality  or  pipeline  health  |  |  **Function**  |  Serverless  compute  for  lightweight  tasks  |  |  **Prepare**  |  No-code  tool  for  data  transformation  |  |  **Sync**  |  Process  of  materializing  ontology  to  queryable  datasets  |   ###  **APPENDIX  E:  CERTIFICATION  CHECKLIST**   **Before  Taking  Exam:**  -  [  ]  Completed  at  least  3  full  Foundry  projects  -  [  ]  Built  production  pipeline  with  error  handling  -  [  ]  Implemented  data  quality  framework  -  [  ]  Set  up  monitoring  and  alerts  -  [  ]  Practiced  ontology  modeling  -  [  ]  Taken  2+  practice  exams  -  [  ]  Reviewed  all  official  documentation  -  [  ]  Scheduled  exam  at  optimal  time   **Exam  Day  Checklist:**  -  [  ]  Government-issued  ID  ready  -  [  ]  Workspace  cleared  (proctoring  requirements)  -  [  ]  Computer  fully  charged  +  charger  available  -  [  ]  Internet  connection  stable  -  [  ]  Water  bottle  nearby  -  [  ]  15  minutes  early  for  check-in  -  [  ]  Positive  mindset  established   ---   ##  **FINAL  WORDS  OF  WISDOM**   ###  **1.  Think  Like  a  Product  Builder**  You're  not  just  building  pipelines;  you're  creating  data  products.  Consider:  -  Who  are  your  users?  -  What  problems  do  you  solve?  -  How  do  you  ensure  reliability?  -  How  do  you  measure  success?   ###  **2.  Embrace  Foundry's  Philosophy**  -  **Version  everything**  -  reproducibility  is  power  -  **Model  relationships**  -  data  in  context  is  valuable  -  **Automate  quality**  -  trust  enables  speed  -  **Collaborate  widely**  -  break  down  silos   ###  **3.  Continuous  Learning  Path**  1.  **Foundational:**  Master  the  basics  (complete)  2.  **Advanced:**  Deep  dive  into  performance  and  scale  

3.  **Expert:**  Lead  complex  implementations  4.  **Architect:**  Design  organization-wide  solutions   ###  **4.  Certification  is  a  Milestone,  Not  Destination**  The  exam  validates  knowledge,  but  real  expertise  comes  from:  -  Building  and  breaking  things  -  Learning  from  failures  -  Teaching  others  -  Staying  curious   ###  **5.  Remember  Why  This  Matters**  Every  pipeline  you  build,  every  dataset  you  clean,  every  ontology  you  design  helps  someone  
make
 
better
 
decisions.
 
You're
 
enabling:
 -  Faster  business  insights  -  More  accurate  predictions  -  Better  customer  experiences  -  Smarter  strategic  choices   **You're  not  just  a  data  engineer.  You're  a  translator  between  raw  data  and  business  value.**   ---   ##  **CONTACT  &  COMMUNITY**   **Official  Resources:**  -  Palantir  Foundry  Documentation  -  Foundry  Community  Forums  -  Official  Training  Programs  -  Certification  Study  Guide   **Practice  Environments:**  -  Foundry  Training  Instances  -  Community  Sandboxes  -  Open  Datasets  for  Practice  -  Sample  Projects  Repository   **Stay  Updated:**  -  Release  Notes  (quarterly  updates)  -  Best  Practices  Guides  -  Case  Studies  -  User  Group  Meetings   ---   **GOOD  LUCK  ON  YOUR  CERTIFICATION  JOURNEY!**   *May  your  pipelines  always  run  green,  your  data  always  be  clean,  and  your  ontology  always  
be
 
meaningful.*
 

 **-  The  Foundry  Architect**   

Palantir Data Engineering Certification
Volume 2 â€” Transforms, Incremental Processing & Business
Logic
How to Use This Volume
This volume is written as  print-ready textbook material. You should be able to: - Read it linearly -
Annotate margins - Refer back during revision
This volume focuses on the  core skill Palantir evaluates most aggressively: your ability to design
correct, reproducible, production-safe transformations in Foundry.
If you understand this volume deeply, you will be able to reason through most scenario-based exam
questions.
Chapter 5 â€” Transforms: The Heart of Foundry
5.1 What a Transform Is (Precise Definition)
In Foundry, a transform is not merely a script or query.
A transform is: - A pure, deterministic function - That consumes one or more versioned datasets - And
produces a new, immutable dataset version
Formally:
Output Dataset Version = f(Input Dataset Versions, Transform Code, Configuration)
This definition has three critical implications: 1. The same inputs always produce the same output 2.
Outputs are reproducible at any point in the future 3. Transforms can be reasoned about independently
If any solution breaks determinism, it breaks Foundryâ€™s model.
5.2 Why Foundry Forces Explicit Transforms
In many data platforms, engineers: - Run ad-hoc queries - Overwrite tables - Debug live data
Foundry  explicitly forbids this because it makes: - Auditing impossible - Rollbacks unreliable - Root-
cause analysis ambiguous
Transforms exist to ensure: - Every change is attributable - Every output is explainable - Every error is
traceable
1

Exam  Insight: If  a  proposed  solution  allows  silent  mutation  or  undocumented  logic,  it  is  almost
certainly wrong.
Chapter 6 â€” Types of Transforms (Deep, Comparative
Understanding)
6.1 SQL Transforms
What SQL Transforms Are Best At
SQL transforms are ideal when: - Logic is declarative - Business rules must be transparent - Non-
engineers may review logic
Examples: - Filtering invalid records - Aggregations - Dimension enrichment - Standard joins
Why Palantir Favors SQL When Possible
SQL has three properties Palantir values: 1. Readability â€” intent is obvious 2. Governability â€” logic is
reviewable 3. Determinism â€” fewer hidden side effects
Limitations of SQL Transforms
SQL is not ideal for: - Complex procedural logic - Stateful algorithms - Advanced custom processing
Exam Judgment Rule: If SQL can express the logic clearly, SQL is the preferred answer .
6.2 Code Transforms (Python / PySpark)
When Code Is Necessary
Code transforms are appropriate when: - Logic is algorithmic - Multiple processing steps are required -
Performance tuning is needed
Examples: - Complex deduplication - Sessionization - Feature engineering
Responsibilities That Come With Code
With power comes responsibility. Code transforms require you to: - Explicitly manage schemas - Handle
nulls and edge cases - Ensure deterministic output
Foundry does not protect you from bad code.
Exam Trap: Choosing code â€œbecause it is more flexibleâ€ without justification is incorrect.
2

6.3 Pipeline Builder (Low-Code Transforms)
What Pipeline Builder Is For
Pipeline Builder is designed for: - Simple joins - Filters - Projections
It is intentionally constrained.
Why It Exists
Pipeline Builder allows: - Faster iteration - Broader participation
But it is not a replacement for engineering judgment.
Exam Insight: Pipeline Builder is rarely the correct answer for complex or critical logic.
Chapter 7 â€” Incremental Processing (The Most Important
Chapter)
7.1 Why Incremental Processing Exists
Incremental processing exists to: - Reduce compute cost - Improve freshness - Scale pipelines
However , it introduces correctness risk.
Foundry treats incremental logic as opt-in complexity, not a default.
7.2 The Three Preconditions for Safe Incremental Logic
Incremental processing is only safe if all three conditions hold:
Stable Primary Key
Each real-world entity maps to exactly one key
Keys never change
Reliable Change Detection
Timestamps, version numbers, or CDC logs
Idempotent Writes
Reprocessing the same input does not duplicate data
If any condition fails, the pipeline must be full-refresh.
1. 
2. 
3. 
4. 
5. 
6. 
7. 
3

7.3 Common Incremental Patterns
Append-Only
New rows are added
Existing rows never change
Safe only when: - Data is immutable by nature (e.g., event logs)
Merge / Upsert
Existing rows may be updated
Requires careful deduplication
This is where most bugs occur .
7.4 Late-Arriving and Corrected Data
Late data breaks naive incremental logic.
Correct handling requires: - Identifying affected historical windows - Reprocessing those windows -
Preserving original raw records
Exam  Pattern: Questions  often  describe  late  data  implicitly.  The  correct  answer  always  preserves
historical truth.
Chapter 8 â€” Deduplication (Often Underestimated)
8.1 Why Deduplication Is Hard
Duplicates arise from: - Retries - Source bugs - Event replay
Naive deduplication: - Drops valid records - Loses corrections
8.2 Correct Deduplication Strategy
A correct strategy: 1. Define the business key 2. Define recency or priority 3. Preserve all raw evidence
Deduplication belongs in curated layers, never raw.
Chapter 9 â€” Business Logic Placement
9.1 What Is Business Logic?
Business logic includes: - Classification rules - Thresholds - Eligibility criteria
â€¢ 
â€¢ 
â€¢ 
â€¢ 
4

It encodes organizational belief, not fact.
9.2 Where Business Logic Must Live
Business logic must: - Live in curated transforms - Be version-controlled - Be reviewable
Never embed business logic in: - Raw ingestion - Ontology actions (unless explicitly required)
Chapter 10 â€” Anti-Patterns (Exam Gold)
10.1 Common Mistakes
Cleaning data during ingestion
Using incremental logic without keys
Exposing raw data to users
Overusing code when SQL suffices
Each of these violates Foundry principles.
Chapter 11 â€” How This Appears in the Exam
Typical exam prompts: - â€œDesign a pipeline for changing recordsâ€ - â€œOptimize this pipeline for scaleâ€ - â€œFix
incorrect incremental logicâ€
Correct answers: - Emphasize safety - Preserve lineage - Prefer explicitness
End of Volume 2
Next volumes will cover: - Volume 3: Ontology, semantic modeling, actions - Volume 4: Data quality,
governance, security - Volume 5: Debugging, operations, exam strategy
â€¢ 
â€¢ 
â€¢ 
â€¢ 
5

Palantir Data Engineering Certification
Volume 3 â€” Ontology, Semantic Modeling & Business Actions
How to Use This Volume
This volume explains how Foundry turns data into operational decision systems.
You should read this volume if you want to: - Understand how Palantir expects data to be consumed -
Reason about exam questions involving business users, workflows, or applications - Avoid common
semantic modeling mistakes
Ontology is one of the most differentiating and heavily tested areas of the certification.
Chapter 12 â€” Why the Ontology Exists
12.1 The Problem Ontology Solves
Traditional data platforms stop at datasets and dashboards. They assume that: - Users understand
schemas - Users know how to join data - Users will not misuse fields
In reality: - Business users think in  entities and actions, not tables - Dashboards do not support
operational workflows - Raw datasets are easy to misinterpret
The Ontology exists to close this gap.
It transforms:
â€œRows and columnsâ€ into â€œReal-world objects and decisionsâ€
12.2 Ontology as a Contract
The Ontology is a semantic contract between: - Data engineers - Domain experts - Application builders
Once defined, it guarantees: - Stable meaning - Controlled access - Predictable behavior
Changing ontology semantics is a breaking change, similar to changing an API.
Exam Insight: If a question involves stability, safety, or cross-team usage, ontology-based answers are
often correct.
1

Chapter 13 â€” Core Ontology Concepts (Deep Definitions)
13.1 Objects
An Object represents a real-world entity that: - Has identity - Persists over time - Accumulates state
Examples: - Customer - Order - Asset - Case
An object must: - Have a stable primary key - Map cleanly to curated datasets
Objects are not: - Temporary aggregates - One-off metrics - Derived summaries
13.2 Properties
Properties are attributes of an object.
Examples: - Customer status - Order amount - Asset location
Properties: - Are typed - Can change over time - May come from multiple datasets
Important distinction: - Properties describe state - Metrics describe aggregates
Do not confuse the two.
13.3 Relationships
Relationships define how objects connect.
Examples: - Customer â†’ places â†’ Order - Asset â†’ located at â†’ Site
Good relationships: - Reflect real-world meaning - Are navigable - Preserve referential integrity
Bad relationships: - Encode temporary logic - Exist only for convenience
13.4 Actions
Actions represent business operations, not data transformations.
Examples: - Approve - Flag - Escalate - Assign
Actions: - Change object state - Trigger workflows - Are auditable
Actions are not: - Batch transformations - Data cleanup jobs
2

Chapter 14 â€” Mapping Datasets to Ontology
14.1 Curated Data as the Source of Truth
Ontology should be backed by: - Curated datasets only - Never raw datasets
Why: - Raw data may be inconsistent - Business meaning is unresolved - Quality checks are incomplete
The ontology assumes trustworthy inputs.
14.2 One Object, Multiple Datasets
An object may draw properties from: - A base entity table - Status change logs - Derived enrichment
datasets
This is acceptable if: - Identity remains stable - Semantics are clear
Do not fragment object identity.
14.3 Handling Slowly Changing Attributes
Some properties: - Change rarely - Must be historically correct
Correct handling: - Preserve change history - Expose current state clearly
Never overwrite historical meaning silently.
Chapter 15 â€” Ontology Design Principles (Exam-Critical)
15.1 Domain-Driven Modeling
Ontology should reflect: - Business language - Domain concepts - User mental models
Avoid: - Technical naming - Source-system leakage
Ontology is for humans, not pipelines.
15.2 Minimal but Complete
Good ontology design: - Exposes what users need - Hides what they donâ€™t
Overly rich ontology: - Confuses users - Increases maintenance risk
Under-modeled ontology: - Forces users back to datasets
3

15.3 Stability Over Optimization
Ontology should change: - Slowly - Deliberately
Frequent changes: - Break applications - Undermine trust
Exam Insight: If a solution requires frequent ontology changes, it is likely wrong.
Chapter 16 â€” Ontology and Security
16.1 Ontology as an Access Boundary
Ontology enforces: - Object-level permissions - Property-level visibility - Action-level authorization
This allows: - Safe self-service - Controlled exposure
Users interact with objects, not tables.
16.2 Why Ontology Is Safer Than Direct Dataset Access
Direct dataset access: - Requires schema knowledge - Risks misuse - Exposes sensitive fields
Ontology: - Abstracts complexity - Enforces policy - Limits blast radius
This is why Palantir strongly favors ontology-driven access.
Chapter 17 â€” Common Ontology Anti-Patterns
17.1 Treating Ontology as Metadata Only
This leads to: - Underpowered applications - Duplicate logic elsewhere
Ontology is executable, not descriptive.
17.2 Encoding Business Logic in Actions Only
Actions should: - Trigger workflows - Change state
They should not: - Replace curated transformations
Business logic belongs primarily in data pipelines.
4

17.3 Modeling Metrics as Objects
Metrics: - Are aggregates - Change with context
Objects: - Represent entities
Confusing the two causes semantic errors.
Chapter 18 â€” How Ontology Appears in the Exam
Typical scenarios: - â€œEnable business users to interact safely with dataâ€ - â€œExpose curated data without
schema knowledgeâ€ - â€œBuild operational workflowsâ€
Correct answers: - Introduce ontology objects - Use actions for decisions - Restrict raw access
End of Volume 3
Next  volumes:  -  Volume  4:  Data  quality,  governance,  security,  lineage  -  Volume  5:  Debugging,
operations, and exam reasoning
5

Palantir Data Engineering Certification
Volume 1 â€” Foundations, Architecture & Data Ingestion
How to Use This Volume
This volume establishes the foundational mental model required to understand Palantir Foundry and
to succeed in the Data Engineering Certification exam.
It is written as  textbook-style reading material, not notes. You should read it sequentially. Later
volumes assume you fully understand the concepts defined here.
If you internalize this volume, you will understand why Foundry works the way it does â€” which is the
single most important factor in passing the exam.
Chapter 1 â€” What Problem Foundry Was Built to Solve
1.1 Why Traditional Data Platforms Fall Short
Most data platforms evolved to solve one of two problems:
Analytics â€” reporting, dashboards, BI
Computation â€” large-scale batch or distributed processing
These  platforms  assume:  -  Data  is  mostly  static  -  Errors  are  tolerable  -  Consumers  are  technically
sophisticated
Palantir Foundry was built for a fundamentally different environment.
It was designed for organizations where: - Decisions are operational, not just analytical - Errors have
legal, financial, or human consequences - Data must be explained, audited, and defended
In such environments, the question is not:
â€œCan we compute this?â€
But rather:
â€œCan we explain why this result exists, months or years later?â€
Foundry is optimized for accountability, not convenience.
1. 
2. 
1

1.2 Foundry as a Data Operating System
Foundry is best understood as a data operating system, not a tool.
Like  an  operating  system,  it:  -  Enforces  rules  by  default  -  Prevents  unsafe  behavior  -  Makes  safe
behavior the path of least resistance
Foundry does not trust engineers to â€œdo the right thingâ€ consistently. Instead, it bakes correctness into
the architecture.
This design philosophy explains many Foundry constraints that feel heavy to engineers accustomed to
ad-hoc systems.
Chapter 2 â€” Foundryâ€™s Core Design Principles (Exam-Critical)
Every Foundry feature enforces a small set of non-negotiable principles. Exam questions are almost
always testing whether you understand these principles.
2.1 Immutability
In Foundry, datasets are immutable.
Once a dataset version is created: - It is never modified - Corrections produce a new version
This is not a technical limitation. It is a deliberate design choice.
Immutability enables: - Historical audits - Time-travel debugging - Reproducibility of past decisions
If data could be overwritten, none of these would be possible.
Exam implication: Any approach that mutates existing data is almost always incorrect.
2.2 Lineage as a First-Class Concept
Every dataset in Foundry records: - Its upstream inputs - The exact transformation logic - The version of
each dependency
Lineage is not documentation. It is executable knowledge.
Lineage enables: - Impact analysis - Root cause investigation - Controlled change management
If you cannot trace how a value was produced, Foundry considers the system unsafe.
2

2.3 Explicitness Over Convenience
Foundry consistently prefers: - Explicit schemas over inferred ones - Explicit dependencies over dynamic
discovery - Explicit permissions over implicit access
Implicit behavior is convenient in small systems, but dangerous at scale.
Explicit systems are: - Easier to debug - Easier to audit - Safer to evolve
Exam implication: â€œAuto-magicâ€ solutions are usually wrong.
2.4 Separation of Concerns
Foundry enforces strict separation between: - Raw data - Business logic - Semantic meaning - Access
control
Each concern lives in a different layer . Mixing them creates fragility and governance risk.
Chapter 3 â€” Foundry Architecture: The Layered Model
3.1 Thinking in Responsibility Zones
Foundry architecture should be understood as layers of responsibility, not tools or technologies.
External World (Untrusted)
â†“
Ingestion Layer (Evidence Capture)
â†“
Raw Data Layer (Historical Truth)
â†“
Transformation Layer (Business Logic)
â†“
Curated Data Layer (Business Truth)
â†“
Ontology (Semantic Contract)
â†“
Applications & Decisions
Each layer answers a fundamentally different question.
3.2 Raw Data Layer â€” Capturing Evidence
The raw data layer exists to answer:
â€œWhat did the source system send us, exactly as it was?â€
3

Raw datasets: - Preserve all fields - Preserve original types - Preserve original errors
Raw data is treated like evidence in a legal case.
If the source system was wrong, the raw data must reflect that wrongness.
Raw data is never: - Cleaned for business use - Corrected - Rewritten
3.3 Curated Data Layer â€” Defining Business Truth
Curated datasets answer:
â€œGiven the raw evidence, what does the organization believe is correct?â€
This is where: - Data is cleaned - Types are enforced - Duplicates are resolved - Business rules are
applied
Curated data is safe for broad consumption.
3.4 Ontology â€” The Human Interface
The ontology layer translates curated data into: - Real-world entities - Relationships - Actions
Ontology allows non-technical users to interact with data safely, without understanding schemas or
joins.
Chapter 4 â€” Data Ingestion as Evidence Engineering
4.1 What Ingestion Means in Foundry
In many systems, ingestion is treated as a plumbing problem.
In Foundry, ingestion is treated as an evidence capture problem.
Your responsibility during ingestion is: - Faithfulness to the source - Auditability - Reprocessability
Speed is secondary to correctness.
4.2 Batch Ingestion
Batch ingestion captures: - Full snapshots, or - Periodic deltas
Advantages: - Simple mental model - Deterministic reprocessing - Easier audits
Disadvantages: - Higher latency - Higher compute cost
4

Batch ingestion is preferred when correctness is more important than freshness.
4.3 Incremental Ingestion
Incremental ingestion captures only new or changed records.
It requires: - Stable primary keys - Reliable change detection - Idempotent writes
Without all three, incremental ingestion introduces silent corruption.
Foundry treats incremental ingestion as optional complexity, not a default.
4.4 Late-Arriving Data
Late-arriving data occurs when: - Events arrive out of order - Corrections arrive days or weeks later
Correct  handling  requires:  -  Preserving  original  raw  records  -  Reprocessing  affected  downstream
windows
Overwriting history is never acceptable.
4.5 Schema Drift
Schema drift is expected in real systems.
Foundryâ€™s approach: - Allow drift in raw ingestion - Enforce schema in curated layers - Fail loudly when
business logic breaks
Silently adapting schemas is dangerous.
Chapter 5 â€” Why Naive Designs Fail the Exam
Common incorrect instincts include: - Cleaning data during ingestion - Updating records in place -
Exposing raw data to users - Collapsing layers for simplicity
These approaches violate Foundryâ€™s core principles.
The exam is designed to surface these mistakes.
End of Volume 1
Next volumes build on this foundation: - Volume 2: Transforms and incremental processing - Volume 3:
Ontology and semantic modeling - Volume 4: Governance, security, and lineage - Volume 5: Debugging,
operations, and exam strategy
5

Palantir Data Engineering Certification
Volume 4 â€” Data Quality, Governance, Security & Lineage
How to Use This Volume
This volume is written to exactly match the format, tone, and depth of Volume 3.
It is: - Conceptual, not tool-manualâ€“like - Explanatory rather than procedural - Focused on why Foundry
works this way, not just what to do
You should read this volume as a continuation of the semantic and operational model introduced by the
Ontology.
Chapter 19 â€” Why Governance Exists in Foundry
19.1 Governance Is an Architectural Concern
In many data platforms, governance is treated as an external constraint imposed by policy, compliance
teams,  or  documentation.  In  Foundry,  governance  is  treated  as  an  architectural  property  of  the
system itself.
Foundry was designed for environments where: - Data is shared across many teams - Decisions have
operational consequences - Mistakes must be explainable after the fact
In such environments, relying on human discipline is insufficient. Governance must be  enforced by
default, not requested as a favor .
19.2 Governance vs Control
Governance is often confused with control.
Control restricts behavior . Governance enables safe autonomy.
Foundryâ€™s goal is not to prevent users from accessing data, but to ensure that whatever access they do
have is: - Appropriate - Auditable - Reversible
This  distinction  explains  why  Foundry  emphasizes  lineage,  immutability,  and  layered  permissions
instead of centralized approval workflows.
1

Chapter 20 â€” Data Quality as a Property of Curated Data
20.1 What â€œData Qualityâ€ Means in Foundry
In Foundry, data quality is not defined by cleanliness alone. High-quality data is data that: - Can be
trusted for decision-making - Has clearly defined assumptions - Can be reproduced and audited
Quality is therefore contextual, not absolute.
20.2 Raw Data Is Allowed to Be Wrong
Raw datasets exist to preserve evidence. As such, they are allowed to contain: - Missing values - Invalid
records - Duplicates - Inconsistent schemas
Correcting raw data would destroy evidence and undermine auditability.
Raw data answers the question:
â€œWhat did the source system say?â€
Not:
â€œWhat should the data have said?â€
20.3 Curated Data Must Be Defensible
Curated datasets represent organizational belief. They answer:
â€œGiven the evidence, what do we believe to be correct?â€
As a result, curated datasets must: - Enforce schema and types - Resolve duplicates - Handle invalid
records explicitly - Encode business rules transparently
Silently passing bad data downstream is a governance failure.
Chapter 21 â€” Validation and Failure Semantics
21.1 Validation Is a Design Decision
Validation  is  not  simply  about  catching  errors.  It  is  about  deciding  what  should  happen  when
assumptions are violated.
Every validation rule should answer:
â€œWhat is the risk if this data is wrong?â€
2

21.2 Fail-Fast Semantics
Fail-fast validation is appropriate when: - Data feeds automated decisions - Incorrect values would
propagate harm - Regulatory or financial accuracy is required
In these cases, pipeline failure is safer than silent corruption.
21.3 Fail-Soft Semantics
Fail-soft  validation  may  be  acceptable  when:  -  Data  is  exploratory  -  Availability  is  prioritized  over
correctness - Errors can be tolerated temporarily
However , fail-soft behavior must still be observable. Hidden errors are unacceptable.
Chapter 22 â€” Access Control as a Layered System
22.1 Why Access Control Is Layered
No single access control mechanism is sufficient for complex organizations.
Foundry therefore enforces permissions at multiple levels: - Dataset - Column - Row - Ontology object -
Ontology action
Each layer reduces risk and limits blast radius.
22.2 Dataset-Level Permissions
Dataset-level permissions control who can: - Read data - Write data - Promote changes
Raw datasets typically have restricted readership. Curated datasets are more broadly accessible but
more strictly governed.
22.3 Column- and Row-Level Security
Column-level security protects sensitive attributes such as PII. Row-level security restricts which records
a user can see.
These controls allow safe data sharing without duplication.
Chapter 23 â€” Ontology as a Governance Boundary
23.1 Why Ontology Is Safer Than Tables
Tables expose schemas, join logic, and internal representations.
3

Ontology exposes: - Business entities - Meaningful properties - Allowed actions
This abstraction dramatically reduces misuse and misinterpretation.
23.2 Ontology Permissions
Ontology  permissions  allow  organizations  to:  -  Control  who  can  view  entities  -  Restrict  sensitive
properties - Limit who can execute actions
This enables self-service without sacrificing governance.
Chapter 24 â€” Lineage and Organizational Memory
24.1 What Lineage Captures
Lineage captures: - Upstream dependencies - Transformation logic - Dataset versions
It represents the organizational memory of how data came to exist.
24.2 Why Lineage Is Central to Trust
When data changes, lineage allows teams to answer: - What changed? - Why did it change? - Who is
affected?
Without lineage, trust erodes rapidly.
Chapter 25 â€” Governance Anti-Patterns
25.1 Common Mistakes
Common governance failures include: - Exposing raw data directly to business users - Encoding access
logic in code - Relying on documentation instead of enforcement
These patterns scale poorly and are frequently tested in the exam.
End of Volume 4
This volume completes the governance model that underpins Foundry. The next volume focuses on
operating and debugging systems built on these principles.
4

Palantir Data Engineering Certification
Volume 5 â€” Debugging, Operations & Exam Reasoning
(Expanded Edition)
How to Use This Volume
This expanded edition of Volume 5 provides  greater depth, nuance, and reasoning detail than the
earlier version, while preserving the conceptual, textbook-style format of Volumes 3 and 4.
This volume is intentionally written to help you: - Think like a production data engineer, not a pipeline
builder - Reason about  failure, change, and uncertainty - Eliminate incorrect answers in scenario-
based exam questions
Most  candidates  fail  the  Palantir  Data  Engineering  Certification  because  they  underestimate  the
importance of operations and judgment. This volume is designed to correct that.
Chapter 26 â€” Foundry as a Living Production System
26.1 Why Production Is the Default State
In Foundry, the moment a pipeline is created, it is already a production system.
Unlike experimental data environments, Foundry assumes that: - Data will be consumed by others -
Decisions may be automated - Mistakes may propagate rapidly
As a result, Foundry does not distinguish sharply between â€œdevelopmentâ€ and â€œproductionâ€ in the way
many platforms do. Every dataset version is potentially consumable, traceable, and auditable.
This assumption fundamentally changes how engineers must think about system design.
26.2 Systems Are Defined by Their Failure Modes
A systemâ€™s true behavior is revealed not when everything works, but when assumptions break.
Foundry is designed so that when failures occur , they are: - Explicit rather than silent - Local rather than
systemic - Explainable rather than mysterious
Any design that allows silent corruption is considered unsafe, regardless of how efficient it appears.
This philosophy underpins Foundryâ€™s insistence on immutability, lineage, and explicit dependencies.
1

Chapter 27 â€” Common Failure Modes in Foundry Pipelines
27.1 Upstream Schema Evolution
Upstream systems evolve continuously. New fields are added, data types change, and semantics shift.
Correct Foundry behavior is not to prevent change, but to contain its impact.
Foundry expects: - Raw ingestion to accept schema drift - Curated transforms to detect assumption
violations - Failures to surface early and loudly
Automatically adapting downstream logic to schema changes hides risk and undermines trust.
27.2 Semantic Drift and Business Logic Decay
Not all failures are technical. Over time, business meaning itself can drift.
Examples include: - Changes in classification rules - New regulatory definitions - Evolving operational
thresholds
If business logic is hard-coded implicitly or scattered across systems, such changes become dangerous.
Foundry  mitigates  this  risk  by  centralizing  business  logic  in  curated  transforms  and  ontology
definitions.
27.3 Incremental Processing as a Source of Latent Failure
Incremental pipelines often appear correct initially but fail under edge conditions.
Common causes include: - Late-arriving data - Replayed events - Corrections to historical records
These failures are particularly dangerous because they may only surface after significant delay, at which
point downstream trust has already been compromised.
This is why Foundry treats incremental logic as an optimization that must be explicitly justified.
Chapter 28 â€” Debugging Through Lineage and Time
28.1 Why Traditional Debugging Fails
In mutable systems, debugging often relies on inspecting the current state and attempting to infer
what went wrong.
In Foundry, this approach is insufficient and unnecessary.
2

Because every dataset is immutable and versioned, engineers can directly examine: - Previous correct
states - The exact change that introduced failure - The full dependency graph affected
Debugging becomes an exercise in comparison, not speculation.
28.2 Time Travel as a First-Class Capability
Time travel allows engineers to: - Reproduce historical outputs - Validate hypotheses about failures -
Confirm whether changes were intentional
This capability is essential not only for debugging, but for maintaining long-term organizational trust in
data systems.
Chapter 29 â€” Operating for Reliability and Trust
29.1 Reliability Is About Expectations, Not Perfection
A reliable system is not one that never fails, but one whose failures are predictable and understandable.
Foundry encourages engineers to design systems where: - Data freshness expectations are explicit -
Quality guarantees are documented in code - Failure modes are visible to stakeholders
Hidden fragility erodes trust faster than visible failure.
29.2 Monitoring as Organizational Feedback
Monitoring signals should be treated as feedback from the system.
Key signals include: - Pipeline execution success - Data latency - Volume and distribution anomalies
Ignoring these signals allows small issues to grow into systemic failures.
Chapter 30 â€” Managing Change in Foundry
30.1 Why Change Is Inevitable
No data system remains static. Sources change, requirements evolve, and scale increases.
Foundry assumes constant change and therefore emphasizes: - Versioned code - Controlled promotion -
Explicit dependency management
Systems that do not plan for change inevitably fail catastrophically.
3

30.2 Minimizing Blast Radius
Safe systems minimize the impact of change.
Foundry  achieves  this  by:  -  Enforcing  separation  of  concerns  -  Encouraging  narrow,  composable
transforms - Making dependencies explicit
When something breaks, only what depends on it should be affected.
Chapter 31 â€” How the Certification Exam Evaluates You
31.1 The Exam Tests Judgment Under Uncertainty
The Palantir Data Engineering Certification exam is not a test of memorization.
It evaluates whether you can: - Recognize unsafe shortcuts - Choose conservative, defensible designs -
Reason about long-term consequences
Many incorrect answers appear attractive because they optimize for speed or simplicity.
31.2 Identifying Wrong Answers Quickly
Wrong  answers  often  share  common  traits:  -  They  mutate  data  in  place  -  They  hide  errors  or
assumptions - They collapse architectural layers - They prioritize performance without guarantees
Learning to recognize these patterns dramatically improves exam performance.
Chapter 32 â€” A Practical Reasoning Framework
When facing an unfamiliar exam scenario, ask:
Does this preserve historical truth?
Can this outcome be reproduced?
Are failure modes explicit?
Is governance enforced by design?
Is the solution safe for non-experts?
If the answer to any is â€œno,â€ the option is almost certainly incorrect.
Chapter 33 â€” Integrating All Volumes
This volume completes the conceptual arc of the textbook: - Volume 1 established the mental model -
Volume  2  taught  transformation  mechanics  -  Volume  3  introduced  semantic  meaning  -  Volume  4
enforced governance and trust - Volume 5 explains how systems behave over time
1. 
2. 
3. 
4. 
5. 
4

Together , these volumes teach you how to think like a Foundry data engineer .
End of Volume 5
This expanded edition provides the depth required to reason confidently about production systems and
exam scenarios.
5